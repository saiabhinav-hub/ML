---
title: "smandumu_ml3_Hierarchical Clustering"
author: "smandumu"
date: "11/16/2019"
output: html_document
---
```{r}
Cereals <- read.csv("Cereals.csv")
Cereals
# Remove all cereals with missing values
set.seed(123)
C <- na.omit(Cereals)
C
C <- subset( C, select = -c(1,2,3))
#View(C)
C <-as.data.frame(scale(C))
str(C)
# Question a
library(factoextra)    #clustering visualization
library(cluster)       #clustering algorithms
library(NbClust)       #determine optimal no. of clusters
head(C)
str(C)
#View (C)
HC <- hclust(dist(C,method = "euclidean"),method = "complete")
HC
plot(HC,hang = -1, cex=0.6)

# methods to assess best
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")
# function to compute coefficient
ac <- function(x) {  agnes(C, method = x)$ac}
library(purrr)
map_dbl(m, ac)
# The approach used by Ward describes the best clustering mechanism of the four approaches tested
HCW <- agnes(C, method = "ward")
pltree(HCW, cex = 0.6, hang = -1, main = "Dendrogram of agnes")  #  visualize the dendrogram

#*****End of Question a****#

# Question b

# slicing the dendogram on the longest path, 4 is the optimal level of clusters.
# Cut tree into 4 groups
sub_grp <- cutree(HCW, k = 4)
C2 <- as.data.frame(cbind(C,sub_grp))
# Number of members in each cluster
table(sub_grp)
rect.hclust(HCW, k = 4)
#******End Of Question b****#

#Question c
library(caret)
c1<-na.omit(Cereals)


#NewC <- createDataPartition(c1$calories,p=0.75,list = FALSE)
# Question C-a  
trainC<-c1[1:45,] # Partition A
testC<-c1[46:74,] # Partition B

trainC <- as.data.frame(scale(trainC[-c(1,2,3)]))
testC <- as.data.frame(scale(testC[-c(1,2,3)]))

ac <- function(x) {  agnes(trainC, method = x)$ac}
map_dbl(m, ac)
# The approach used by Ward describes the best clustering mechanism of the four approaches tested
#ac <- function(x) {  agnes(testC, method = x)$ac}
#map_dbl(m, ac)
# The approach used by Ward describes the best clustering mechanism of the four approaches tested
CW <- agnes(trainC,method = "ward")
pltree(CW, cex = 0.6, hang = -1, main = "Dendrogram of agnes")
rect.hclust(CW, k = 4)
#pltree(agnes(scale(testC[,-c(1:3)]),method = "ward"), cex = 0.6, hang = -1, main = "Dendrogram of agnes")
#rect.hclust(agnes(scale(testC[,-c(1:3)]),method = "ward"), k = 4)
# Question C-b
CWcut <- cutree(CW, k = 4)
CWtotal <- as.data.frame( cbind(trainC,CWcut))
CWtotal
CWclust1 <- CWtotal[CWtotal$CWcut==1,]
 colMeans(CWclust1)
CWclust2 <- CWtotal[CWtotal$CWcut==2,]
 colMeans(CWclust2)
CWclust3 <- CWtotal[CWtotal$CWcut==3,]
 colMeans(CWclust3)
CWclust4 <- CWtotal[CWtotal$CWcut==4,]
 colMeans(CWclust4)

CWmeans1 <- rbind(colMeans(CWclust1),colMeans(CWclust2),colMeans(CWclust3),colMeans(CWclust4))
CWmeans1

CWmeans <- subset(CWmeans1,select = -c(CWcut))
CWmeans
# Question C-c
CWRC <- data.frame(Records=seq(1,nrow(testC)),cluster_number=rep(0,nrow(testC)))
for(i in seq(1,nrow(testC)) ){
  C1 <- as.data.frame(rbind(CWmeans,testC[i,]))
  E1 <- as.matrix(get_dist(C1))
  CWRC[i,2] <- which.min(E1[5,-5])
  }
CWRC
cbind(Clust=CWRC$cluster_number,Ori_Clust=C2[46:74,14])
table(CWRC$cluster_number==C2[46:74,14])
# End Of Question c#
# Question d
library(GGally)
library(ggplot2)
library(hrbrthemes)
library(viridis)
#ggparcoord(cbind(c(1:4),CWmeans),columns = 2:14,groupColumn = 1,showPoints = TRUE,title = " Charter of cluster",alphaLines = 0.9) + scale_color_viridis (discrete = TRUE)+theme_ipsum()+theme(plot.title = element_text(size = 10)) 


ggparcoord(CWmeans1,
           columns = 1:13, groupColumn = 14,
           showPoints = TRUE, 
           title = "Cluster Characteristics",
           alphaLines = 0.9
) + 
  scale_color_viridis(discrete=FALSE)
# Based on the characteristics of the cluster, they find that Cluster 1 is the strongest with low calories, high protein, potassium, starch, non-carbs, carbohydrates, top-rated.

###In general,when we use the distance metric algorithm the data should be normalised,because the data characteristics are diverse. Therefore, it is necessary to standardize the data.









```

